
---

### ğŸŸ£ Motif 5 â€” **Clarification par contre-exemple** : *Explorer les limites dâ€™une proposition*

**ğŸ¯ Contexte**
Le LLM a produit une rÃ©ponse satisfaisante â€” un code, une solution technique, une recommandation. Tout semble correctâ€¦ mais une forme de doute persiste. Est-ce vraiment robuste ? La rÃ©ponse couvre-t-elle tous les cas ? Le raisonnement tient-il dans les cas extrÃªmes ?

**ğŸš§ ProblÃ¨me**
Le modÃ¨le donne souvent une solution Â« idÃ©ale Â» ou typique, qui **masque les cas limites ou les situations dâ€™Ã©chec**. Le dÃ©veloppeur peut Ãªtre tentÃ© de faire confiance par dÃ©faut. Pourtant, sans mise Ã  lâ€™Ã©preuve, on risque de dÃ©ployer une solution fragile, biaisÃ©e ou naÃ¯ve.

**âœ… Solution**
Interroger la rÃ©ponse **par la nÃ©gation** : demander un **contre-exemple**, une situation oÃ¹ la solution Ã©choue, devient inefficace ou produit un effet inattendu. Cela rÃ©vÃ¨le les **limites implicites** du raisonnement et affine la comprÃ©hension de ce que la solution couvre â€” ou pas.

> Exemples de prompts :
>
> * Â« Dans quel cas cette solution pourrait Ã©chouer ? Â»
> * Â« Peux-tu proposer un exemple de donnÃ©es qui poserait problÃ¨me ? Â»
> * Â« Et si le fichier est vide ? Si la connexion Ã©choue ? Si lâ€™utilisateur nâ€™est pas authentifiÃ© ? Â»
> * Â« Quelle hypothÃ¨se implicite, si elle est fausse, rend cette solution invalide ? Â»

**ğŸ“Œ ConsÃ©quences**

* DÃ©tection prÃ©coce des cas limites.
* Meilleure robustesse de la solution proposÃ©e.
* Formation dâ€™une posture critique chez le dÃ©veloppeur.
* RÃ©duction des effets de bord ou des surprises en production.
* Enrichissement du prompt initial si besoin (cf. motif 6).

**ğŸ’¡ Exemple dâ€™usage**
Un Ã©tudiant demande au LLM dâ€™implÃ©menter lâ€™algorithme de Dijkstra en JavaScript.
La solution paraÃ®t correcte. Il relance avec :

> *Â« Et si le graphe contient des cycles nÃ©gatifs ? Â»*

Le LLM rÃ©pond :

> *Â« Dijkstra nâ€™est pas adaptÃ© Ã  ce cas. Il faudrait utiliser Bellman-Ford, qui gÃ¨re les poids nÃ©gatifs. Â»*

Cette simple relance transforme une session de gÃ©nÃ©ration en **moment dâ€™apprentissage algorithmique**, en rendant visible une hypothÃ¨se invisible.

**ğŸŒ€ Variantes utiles**

* **Test de bord** : Â« Et si le tableau est vide ? Si une donnÃ©e est nulle ? Â»
* **Stress test** : Â« Et si 10 000 utilisateurs accÃ¨dent Ã  ce module en mÃªme temps ? Â»
* **Contre-rÃ¨gle mÃ©tier** : Â« Quelle situation mÃ©tier invaliderait cette rÃ¨gle ? Â»
* **DÃ©bat simulÃ©** : Â« Peux-tu simuler lâ€™avis dâ€™un dÃ©veloppeur qui critique cette solution ? Â»

**ğŸ› ï¸ Outils associÃ©s**

* Table de tests dâ€™acceptation enrichie par le modÃ¨le.
* Utilisation combinÃ©e avec la gÃ©nÃ©ration de jeux de tests (cf. motif 3).
* Pairing augmentÃ© : un dÃ©veloppeur joue lâ€™avocat du diable avec le LLM.

**ğŸ§  Posture recommandÃ©e**
Ne te satisfais pas de la Â« bonne rÃ©ponse Â» en apparence. Adopte une **posture scientifique** : falsifier, tester, pousser la logique jusquâ€™Ã  ses bords. Câ€™est ainsi que le LLM devient un **partenaire critique**, et non un automate flatteur.

**ğŸ’¬ Prompt-type Ã  mÃ©moriser**

> *Â« Donne un cas qui fait Ã©chouer cette solution. Quâ€™est-ce que cela rÃ©vÃ¨le sur ses limites ? Â»*
