
---

## Chapitre â€” Les motifs du dialogue : construire un langage de conception avec les LLM

> Comment capturer les meilleures pratiques dâ€™interaction avec une IA ? Ce chapitre prÃ©sente une collection de motifs issus du terrain, testÃ©s, rÃ©utilisables et adaptables.

Dans ce chapitre, nous allons explorer les motifs fondamentaux qui Ã©mergent de lâ€™interaction rÃ©guliÃ¨re entre un dÃ©veloppeur et un LLM. Ces motifs sont les unitÃ©s de base dâ€™un nouveau langage de conception, faÃ§onnÃ© non plus uniquement par les frameworks ou les langages de programmation, mais par les rÃ©currences dâ€™usage entre lâ€™humain et le modÃ¨le. Ils forment une bibliothÃ¨que de pratiques rÃ©utilisables, adaptables, que les Ã©quipes peuvent combiner, faire Ã©voluer, et transmettre.

Lâ€™objectif est ici de mettre en lumiÃ¨re des structures de pensÃ©e rÃ©currentes, des scÃ©narios typiques, et des sÃ©quences dâ€™interaction efficaces. Par exemple :

* Comment amorcer un dialogue avec un LLM quand on part dâ€™une idÃ©e floue ?
* Comment affiner un besoin technique Ã  travers plusieurs itÃ©rations ?
* Comment structurer la co-Ã©criture de code, de tests ou de documentation ?
* Comment obtenir une aide sur une impasse ou une panne conceptuelle ?

Ces situations sont autant dâ€™occasions de formaliser des motifs. Nous les prÃ©senterons Ã  travers une fiche type comprenant : nom, contexte, problÃ¨me, solution, consÃ©quences, et exemples. Le tout vise Ã  construire un langage partagÃ© qui outille les praticiens, tout en renforÃ§ant leur posture critique.

### Motif 1 : Â« Question Socratique Â» â€” Reformuler pour comprendre

**Contexte** : Vous Ãªtes confrontÃ© Ã  un besoin exprimÃ© de maniÃ¨re floue ou partielle (ex. : "Je veux faire une API pour envoyer des alertes en cas d'erreur systÃ¨me"), ou vous devez explorer un domaine que vous maÃ®trisez peu.

**ProblÃ¨me** : Un prompt trop vague entraÃ®ne souvent une rÃ©ponse gÃ©nÃ©rique, peu exploitable ou dÃ©connectÃ©e du contexte rÃ©el. Or, les LLM brillent lorsquâ€™ils sont alimentÃ©s en contraintes, en contexte, et en objectifs prÃ©cis.

**Solution** : Engagez un dialogue progressif en reformulant la demande initiale sous forme de questions. Par exemple :

* "Quels types dâ€™erreurs souhaitez-vous capturer ?"
* "Les alertes doivent-elles Ãªtre transmises en temps rÃ©el ou agrÃ©gÃ©es ?"
* "Quel canal de notification privilÃ©gier (email, Slack, webhook) ?"

Cette technique, inspirÃ©e de la maÃ¯eutique socratique, pousse le demandeur (vous-mÃªme ou un collÃ¨gue) Ã  clarifier ses intentions, ce qui amÃ©liore la qualitÃ© du prompt et donc la pertinence de la rÃ©ponse.

**ConsÃ©quences** :

* Le besoin devient plus clair, mÃªme pour les humains impliquÃ©s.
* Le prompt sâ€™enrichit naturellement Ã  chaque itÃ©ration.
* Le LLM agit comme un catalyseur de rÃ©flexion, pas simplement comme un moteur de complÃ©tion.

**Exemple** : Dans un projet d'alerte mÃ©tÃ©o automatisÃ©e, une Ã©quipe a dâ€™abord demandÃ© au LLM de "crÃ©er un script dâ€™envoi dâ€™alertes". La rÃ©ponse obtenue ne convenait pas : elle reposait sur des hypothÃ¨ses erronÃ©es (envoi par email, langage Python, etc.). En reformulant progressivement les besoins (frÃ©quence, canal, gestion des seuils de gravitÃ©, intÃ©gration dans lâ€™architecture existante), le LLM a fini par gÃ©nÃ©rer une solution beaucoup plus proche du besoin rÃ©el. Cette posture a ensuite Ã©tÃ© institutionnalisÃ©e dans lâ€™Ã©quipe sous la forme dâ€™un atelier rÃ©gulier de co-design assistÃ© par LLM.

### Motif 2 : Â« Exploration GuidÃ©e Â» â€” DÃ©composer pour mieux avancer

**Contexte** : Vous devez concevoir une fonctionnalitÃ© complexe, dÃ©couvrir un domaine technique inconnu, ou explorer une solution potentielle dont les tenants et aboutissants vous Ã©chappent encore.

**ProblÃ¨me** : Lorsque vous posez une question large ou trop abstraite Ã  un LLM (ex. : "Comment concevoir une API REST sÃ©curisÃ©e ?"), la rÃ©ponse obtenue peut Ãªtre trop gÃ©nÃ©raliste, peu exploitable, voire source de confusion. Le LLM donne l'impression de Â« savoir tout faire Â» mais sans guider efficacement le cheminement.

**Solution** : Adoptez une posture dâ€™exploration progressive en demandant au LLM de dÃ©couper la tÃ¢che ou le domaine en Ã©tapes claires. Utilisez des formulations comme :

* "Peux-tu me dÃ©crire les grandes Ã©tapes pourâ€¦ ?"
* "Quelle serait une premiÃ¨re version minimale (MVP) ?"
* "Quels choix technologiques sont Ã  envisager, et pourquoi ?"
* "Propose un plan dâ€™implÃ©mentation par Ã©tapes."

Le but est de transformer une montagne confuse en une sÃ©rie de collines franchissables.

**ConsÃ©quences** :

* Le LLM joue un rÃ´le de guide structurant, aidant Ã  prioriser les Ã©tapes.
* La charge cognitive est rÃ©duite, car vous avancez pas Ã  pas.
* Le dialogue devient un levier de formation continue : chaque Ã©tape est une occasion dâ€™apprentissage ciblÃ©.

**Exemple** : Dans un projet de migration dâ€™une application monolithique vers une architecture en microservices, une dÃ©veloppeuse ne savait par oÃ¹ commencer. PlutÃ´t que de demander "Comment faire la migration ?", elle a utilisÃ© lâ€™exploration guidÃ©e : "Peux-tu me proposer un plan en cinq grandes Ã©tapes pour migrer un monolithe vers des microservices, avec les piÃ¨ges courants Ã  Ã©viter ?" Le LLM a alors proposÃ© une approche itÃ©rative, incluant la cartographie des dÃ©pendances, la sÃ©paration des contextes mÃ©tier, la mise en place dâ€™une couche dâ€™API Gateway, et des stratÃ©gies de test spÃ©cifiques. Chaque Ã©tape a ensuite Ã©tÃ© discutÃ©e plus en dÃ©tail, facilitant la prise de dÃ©cision et la coordination avec lâ€™Ã©quipe.

---

> **EncadrÃ© : Une session de pairing augmentÃ©e**
>
> Lors dâ€™une sÃ©ance de co-dÃ©veloppement, deux dÃ©veloppeurs travaillent ensemble Ã  concevoir un module de traitement de factures. Lâ€™un dâ€™eux propose dâ€™interroger le LLM pour structurer le travail. Le prompt initial : *"Comment concevoir un module de traitement de factures dans un ERP ?"* produit une rÃ©ponse dense mais confuse.
>
> Lâ€™un des dÃ©veloppeurs reformule alors : *"Peux-tu me proposer une dÃ©composition en Ã©tapes pour construire ce module, du point de vue fonctionnel et technique ?"*
>
> Le LLM rÃ©pond :
>
> * Identifier les sources de donnÃ©es (factures fournisseurs, clients).
> * DÃ©finir les rÃ¨gles de validation.
> * ModÃ©liser les statuts de traitement.
> * IntÃ©grer les notifications.
> * PrÃ©voir lâ€™export comptable.
>
> Ã€ partir de cette rÃ©ponse, les deux dÃ©veloppeurs rÃ©organisent leur backlog, dÃ©finissent les premiÃ¨res user stories, et rÃ©digent ensemble les spÃ©cifications du MVP. Le LLM a non seulement servi de facilitateur technique, mais aussi de mÃ©diateur de comprÃ©hension mutuelle, rÃ©vÃ©lant des angles morts et clarifiant les prioritÃ©s. Lâ€™exploration guidÃ©e a permis de sortir du flou initial pour entrer dans lâ€™action concrÃ¨te.

---

### Motif 3 : Â« SpÃ©cification InversÃ©e Â» â€” Obtenir un plan Ã  partir du code

**Contexte** : Vous dÃ©couvrez un morceau de code existant, hÃ©ritÃ© ou gÃ©nÃ©rÃ© automatiquement, mais dont les intentions ne sont pas documentÃ©es ou pas comprises. Vous devez le maintenir, le refactorer ou le rÃ©Ã©crire, mais vous manquez de vision sur sa finalitÃ©, ses contraintes ou sa logique mÃ©tier.

**ProblÃ¨me** : Lorsque le code prÃ©cÃ¨de la conception (cas frÃ©quent avec les LLM), il devient difficile dâ€™en dÃ©duire les intentions originales. Cela ralentit la comprÃ©hension, multiplie les erreurs et fragilise la maintenabilitÃ©. Le risque est de bricoler sans fond, ou de produire de la dette technique involontairement.

**Solution** : Utilisez le LLM pour reconstituer les intentions implicites Ã  partir du code : faites-lui "remonter" les spÃ©cifications Ã  partir dâ€™un extrait de code donnÃ©. Câ€™est lâ€™inverse du schÃ©ma habituel (besoin â†’ code) : ici, on produit la documentation ou le plan de test *Ã  partir* du code. Vous pouvez formuler des demandes comme :

* "Peux-tu me dÃ©crire ce que ce code est censÃ© faire, Ã©tape par Ã©tape ?"
* "Quelles rÃ¨gles mÃ©tiers peux-tu dÃ©duire de ce traitement ?"
* "Peux-tu gÃ©nÃ©rer une documentation technique Ã  partir de ce fichier ?"
* "Quelles hypothÃ¨ses ce code semble-t-il faire sur les donnÃ©es ?"

**ConsÃ©quences** :

* Le code devient un point dâ€™entrÃ©e vers la comprÃ©hension mÃ©tier.
* Les Ã©quipes peuvent mieux documenter des systÃ¨mes anciens ou mal maintenus.
* Cela permet de gÃ©nÃ©rer des cas de test Ã  partir de lâ€™implÃ©mentation existante.

**Exemple** : Une dÃ©veloppeuse reprend un ancien script Python dâ€™analyse de logs rÃ©seau, sans documentation ni tests. Elle copie le code dans un prompt et demande au LLM : *"Explique-moi ligne par ligne ce que fait ce script et quelles rÃ¨gles dâ€™analyse il applique."* Le LLM identifie plusieurs Ã©tapes (filtrage par IP, regroupement par heure, dÃ©tection dâ€™anomalies par seuil), quâ€™il reformule sous forme dâ€™un pseudo-algorithme. La dÃ©veloppeuse peut alors gÃ©nÃ©rer une sÃ©rie de tests unitaires, identifier une condition oubliÃ©e (filtres sur protocoles), et dÃ©cider de refactorer le tout sous forme de classes. Le modÃ¨le a jouÃ© ici le rÃ´le de traducteur et de cartographe inversÃ©.

---

> **EncadrÃ© : Un LLM comme auditeur de code**
>
> Lors dâ€™un audit technique sur un systÃ¨me de facturation, une Ã©quipe se retrouve face Ã  un module PHP de plus de 800 lignes, Ã©crit il y a 8 ans, sans tests ni documentation. PlutÃ´t que de l'analyser ligne par ligne, lâ€™Ã©quipe dÃ©cide de le soumettre au LLM par blocs successifs.
>
> Ã€ chaque itÃ©ration, le prompt est : *"Quels traitements rÃ©alise ce bloc de code ? Quelle rÃ¨gle mÃ©tier cela semble-t-il implÃ©menter ?"*
>
> Le LLM identifie la dÃ©tection de doublons, la vÃ©rification de TVA, la gestion dâ€™arrondis, et les cas particuliers de facturation croisÃ©e. Ces Ã©lÃ©ments sont mis en parallÃ¨le avec les processus mÃ©tier dÃ©crits dans la documentation client, rÃ©vÃ©lant plusieurs Ã©carts non documentÃ©s.
>
> Cette approche de spÃ©cification inversÃ©e a permis de :
>
> * Documenter rÃ©troactivement un systÃ¨me critique,
> * RÃ©concilier la logique mÃ©tier et lâ€™implÃ©mentation rÃ©elle,
> * Planifier une refonte progressive sans repartir de zÃ©ro.

---

### ğŸŒ€ Variantes du motif Â« SpÃ©cification InversÃ©e Â»

#### ğŸ“Œ Variante 3.1 : *Reconstruction dâ€™User Stories*

Au lieu de demander uniquement *ce que fait le code*, on pousse le LLM Ã  reformuler les intentions en *termes fonctionnels utilisateur*. Exemple de prompt :

> *Â« En supposant que ce code corresponde Ã  une fonctionnalitÃ© dâ€™un produit, quelle user story pourrait-on en dÃ©duire ? Â»*

**Usage** : utile dans des projets oÃ¹ le code a Ã©tÃ© produit avant la formalisation des besoins (souvent le cas dans des prototypes ou des phases de hackathon).

#### ğŸ“Œ Variante 3.2 : *DÃ©duction des HypothÃ¨ses*

Demandez au LLM :

> *Â« Quelles hypothÃ¨ses implicites ce code semble-t-il faire sur les donnÃ©es, les contextes dâ€™exÃ©cution ou les droits dâ€™accÃ¨s ? Â»*

**Usage** : prÃ©cieux pour dÃ©tecter des biais implicites, des prÃ©supposÃ©s sur les inputs, ou des angles morts en sÃ©curitÃ©.

#### ğŸ“Œ Variante 3.3 : *Contrat implicite*

Demandez :

> *Â« Peux-tu expliciter un contrat dâ€™interface pour cette fonction / ce module (types dâ€™entrÃ©es, sorties, erreurs gÃ©rÃ©es) ? Â»*

**Usage** : aide Ã  produire des *Design by Contract* Ã  posteriori, ou Ã  documenter des API sans doc initiale.

---

### ğŸ›  Mises en pratique concrÃ¨tes

#### âœ… Pratique 1 : GÃ©nÃ©rer les tests Ã  partir de lâ€™implÃ©mentation

Prompt-type :

> *Â« GÃ©nÃ¨re une suite de tests unitaires couvrant les cas normaux, limites et erreurs de cette fonction. Â»*

**RÃ©sultat** : Le LLM suggÃ¨re souvent des cas non couverts dans le code, rÃ©vÃ©lant des lacunes potentielles.

---

#### âœ… Pratique 2 : Ã‰crire la documentation avant refactorisation

Demande :

> *Â« RÃ©dige une documentation technique synthÃ©tique (fonction, paramÃ¨tres, exceptions, effets secondaires) Ã  partir de ce code. Â»*

**Usage** : crÃ©e un point dâ€™ancrage avant transformation, utile pour la relecture et le pair programming.

---

#### âœ… Pratique 3 : DÃ©tecter les effets de bord

Prompt :

> *Â« Est-ce que ce code a des effets secondaires non visibles immÃ©diatement (ex. : Ã©critures sur disque, accÃ¨s Ã  des variables globales, dÃ©pendances rÃ©seau) ? Â»*

**Usage** : audit de code legacy ou critique, base pour migration vers du code pur / testable.

---

### ğŸ”„ IntÃ©gration dans un workflow dâ€™Ã©quipe

Voici une **routine de revue de code assistÃ©e** utilisant ce motif :

1. ğŸ” *SÃ©lectionner une fonction critique Ã  revoir.*
2. ğŸ§  *Demander au LLM de reformuler son intention mÃ©tier.*
3. âœ… *GÃ©nÃ©rer les tests que cette fonction est supposÃ©e passer.*
4. ğŸ”„ *Comparer avec les tests existants.*
5. âœï¸ *Documenter ou enrichir Ã  la volÃ©e.*
6. ğŸ›  *DÃ©cider dâ€™un refactor ou non.*

> ğŸ”§ **Outil compagnon possible** : Un plugin ou script intÃ©grÃ© dans lâ€™IDE qui, Ã  la sÃ©lection dâ€™une fonction, envoie automatiquement un prompt de spÃ©cification inversÃ©e au LLM et affiche les hypothÃ¨ses mÃ©tier dans une fenÃªtre latÃ©rale.

---

### ğŸ§­ Posture recommandÃ©e

* Ne pas se contenter de *ce que le LLM dit*, mais comparer avec les hypothÃ¨ses de lâ€™Ã©quipe.
* Utiliser les rÃ©ponses comme **base de discussion**, notamment avec les parties prenantes non techniques.
* Croiser les approches : spÃ©cification inversÃ©e â†’ exploration guidÃ©e â†’ design dialoguÃ©.

---

### Motif 4 : Â« ModÃ¨le Miroir Â» â€” Comparer plusieurs versions pour Ã©clairer un choix

**Contexte** : Vous hÃ©sitez entre plusieurs implÃ©mentations possibles (par exemple, deux structures dâ€™API, deux algorithmes, deux stratÃ©gies dâ€™architecture), ou vous avez gÃ©nÃ©rÃ© plusieurs variantes avec le LLM et souhaitez les Ã©valuer de faÃ§on argumentÃ©e.

**ProblÃ¨me** : Lorsquâ€™on explore des options seul ou en Ã©quipe, il est facile de se focaliser sur une solution "plausible" sans bien comprendre les diffÃ©rences, les consÃ©quences ou les alternatives. Le LLM, sans guidance, tend Ã  gÃ©nÃ©rer une seule rÃ©ponse par dÃ©faut.

**Solution** : Exploitez le LLM comme un **miroir comparatif**. Demandez-lui explicitement de produire *plusieurs* versions dâ€™une mÃªme solution, puis de **les comparer lui-mÃªme selon des critÃ¨res dÃ©finis**. Câ€™est une mise en tension productive entre options, qui stimule lâ€™analyse critique.

#### Prompt-type :

* *Â« Propose-moi 3 variantes de cette fonction avec des styles ou des structures diffÃ©rentes. Puis compare-les sur lisibilitÃ©, performance, testabilitÃ©. Â»*
* *Â« Voici deux options dâ€™architecture microservices. Peux-tu me les comparer sur les plans de la rÃ©silience, de la scalabilitÃ©, et de la complexitÃ© opÃ©rationnelle ? Â»*

**ConsÃ©quences** :

* On dÃ©centre le raisonnement : ce nâ€™est plus "la bonne solution", mais "la plus adaptÃ©e au contexte".
* Cela entraÃ®ne une meilleure explicitation des critÃ¨res de choix.
* Le LLM devient partenaire dâ€™un raisonnement dialectique, pas seulement un fournisseur de contenu.

---

### ğŸ§ª Exemple concret

Lors dâ€™un projet de refonte de systÃ¨me de paiement, lâ€™Ã©quipe hÃ©site entre :

1. Une architecture orientÃ©e Ã©vÃ©nements avec Kafka.
2. Une architecture RESTful classique avec appels synchrones.

Le LLM est sollicitÃ© via ce prompt :

> *Â« Voici deux options dâ€™architecture. Peux-tu dÃ©tailler les avantages, risques et implications de chacune pour un systÃ¨me Ã  haute disponibilitÃ© devant traiter 100 transactions par seconde ? Â»*

Le modÃ¨le identifie que :

* Kafka apporte rÃ©silience et dÃ©couplage, mais complexifie la supervision.
* REST est plus simple Ã  auditer mais moins robuste en cas de pics de charge ou dâ€™erreurs rÃ©seau.

Cette analyse partagÃ©e permet Ã  lâ€™Ã©quipe de trancher plus sereinement, *en conscience*.

---

> ğŸ§­ **EncadrÃ© : Le LLM comme miroir dâ€™Ã©quipe**
>
> Dans une rÃ©union de design technique, une Ã©quipe ne parvient pas Ã  se mettre dâ€™accord entre deux styles de validation de formulaire cÃ´tÃ© frontend : impÃ©ratif (en JS pur) ou dÃ©claratif (via une lib type Formik ou React Hook Form).
>
> Un dÃ©veloppeur propose de demander au LLM une comparaison structurÃ©e.
>
> Le prompt devient : *Â« Compare les styles impÃ©ratif et dÃ©claratif de validation de formulaire cÃ´tÃ© React. Donne des exemples de code et compare sur maintenabilitÃ©, UX et facilitÃ© de test. Â»*
>
> Le LLM rÃ©pond avec clartÃ©, exemples Ã  lâ€™appui, rÃ©vÃ©lant que le choix dÃ©pend du niveau de complexitÃ© mÃ©tier attendu et de lâ€™organisation du code. Cela permet Ã  lâ€™Ã©quipe de ne pas trancher "par opinion", mais sur des critÃ¨res explicites.
>
> Le modÃ¨le a servi ici de **facilitateur de clarification technique collective**, sans remplacer la dÃ©cision humaine.

---

### ğŸ”„ Variantes du motif Â« ModÃ¨le Miroir Â»

* **Miroir de styles** : comparer plusieurs styles de code pour une mÃªme logique (ex. fonctionnelle vs orientÃ©e objet).
* **Miroir de paradigmes** : comparer des approches (ex. polling vs event-driven).
* **Miroir de technologies** : comparer frameworks, langages, ou bibliothÃ¨ques en fonction dâ€™un usage ciblÃ©.

---

### Motif 5 : Â« Clarification par contre-exemple Â» â€” Explorer les limites d'une proposition

**Contexte** : AprÃ¨s avoir reÃ§u une rÃ©ponse satisfaisante dâ€™un LLM, il arrive que des doutes subsistent : le code proposÃ© est-il robuste ? La solution envisagÃ©e tient-elle dans tous les cas ? Les limites implicites du raisonnement sont-elles comprises ?

**ProblÃ¨me** : Un prompt initialement bien formulÃ© peut conduire Ã  une rÃ©ponse correcte mais trop optimiste ou gÃ©nÃ©rique. Le LLM tend Ã  Â« deviner Â» une solution type, sans toujours prendre en compte les cas limites, les exceptions ou les erreurs de conception possibles. Cela donne lâ€™illusion de maÃ®trise, lÃ  oÃ¹ il faudrait de la rigueur.

**Solution** : Demander explicitement un contre-exemple ou une situation qui ferait Ã©chouer la solution proposÃ©e. Cette approche vise Ã  forcer la mise en lumiÃ¨re de zones grises, dâ€™angles morts ou de prÃ©supposÃ©s implicites. On peut formuler par exemple :

* "Quel cas dâ€™usage pourrait faire Ã©chouer cette architecture ?"
* "Donne-moi un exemple oÃ¹ ce code produit un comportement inattendu."
* "Y a-t-il un scÃ©nario qui rendrait ce modÃ¨le inefficace ?"

Cette maniÃ¨re de questionner pousse le modÃ¨le (et le praticien) Ã  rÃ©flÃ©chir en creux, par la nÃ©gation ou lâ€™invalidation.

**ConsÃ©quences** :

* Renforce la robustesse de la solution en mettant Ã  lâ€™Ã©preuve ses fondements.
* Favorise une posture critique et antifragile dans la conception.
* Transforme le LLM en simulateur d'obstacles potentiels, utile pour la revue de code ou la documentation des limites dâ€™un systÃ¨me.

**Exemple** : Dans un atelier sur la gÃ©nÃ©ration dâ€™algorithmes de parcours de graphe, un Ã©tudiant a demandÃ© Ã  ChatGPT de lui fournir une version optimisÃ©e de Dijkstra en JavaScript. Le code gÃ©nÃ©rÃ© semblait parfaitement fonctionnel. En testant ensuite la robustesse du rÃ©sultat via la question : "Et si le graphe est orientÃ© avec des cycles nÃ©gatifs ?", le LLM a admis que lâ€™algorithme proposÃ© ne convenait pas, et a suggÃ©rÃ© de basculer sur Bellman-Ford avec un test d'intÃ©gritÃ© des poids. Cette interaction a permis non seulement dâ€™enseigner les limites dâ€™un algorithme, mais aussi dâ€™aiguiser lâ€™esprit critique face Ã  la "bonne rÃ©ponse".

---

> ğŸ“Œ **EncadrÃ© â€” Posture rÃ©flexive : oser douter du bon Ã©lÃ¨ve**
>
> Lâ€™une des illusions les plus tenaces dans lâ€™usage des LLM est celle de la "rÃ©ponse parfaite" dÃ¨s la premiÃ¨re itÃ©ration. Un motif comme "Clarification par contre-exemple" invite Ã  adopter une posture scientifique : tester, falsifier, chercher ce qui ne va pas, mÃªme quand tout semble aller bien. Cela sâ€™apparente Ã  une relecture interne du raisonnement â€” une forme de revue de code dialoguÃ©e â€” oÃ¹ le dÃ©veloppeur devient enquÃªteur des failles possibles. Câ€™est aussi une maniÃ¨re de former les plus jeunes Ã  ne pas confondre autoritÃ© de lâ€™outil et vÃ©ritÃ© absolue.

Voici une **fiche de motif** associÃ©e au *Test-Driven Prompting*, conforme Ã  la structure utilisÃ©e dans les chapitres prÃ©cÃ©dents :

---

### Motif 6 : Â« Prompt PilotÃ© par les Tests Â» â€” DÃ©finir les attentes avant dâ€™Ã©crire

**Contexte**
Vous devez rÃ©diger un prompt pour un usage rÃ©current (gÃ©nÃ©ration de code, reformulation de besoin, structuration de contenuâ€¦), mais vous obtenez des rÃ©ponses fluctuantes, incomplÃ¨tes ou peu adaptÃ©es. Vous voulez rendre votre prompt plus fiable, explicite et partageable.

**ProblÃ¨me**
Un prompt Ã©crit "au feeling" donne des rÃ©sultats alÃ©atoires. Sans critÃ¨res de qualitÃ© explicites, il est difficile de dire si la rÃ©ponse du LLM est satisfaisante ou non. Cela rend les itÃ©rations peu reproductibles et lâ€™apprentissage limitÃ©.

**Solution**
Adoptez une dÃ©marche inspirÃ©e du Test-Driven Development : avant de rÃ©diger le prompt, Ã©crivez un ou plusieurs **tests dâ€™intention** qui dÃ©crivent ce que vous attendez du LLM. Ce peuvent Ãªtre :

* des exemples de sortie ;
* des critÃ¨res formels (format, style, structure) ;
* des contraintes de contexte ou de contenu.

Utilisez ensuite ces tests pour guider lâ€™Ã©criture du prompt. Faites-le Ã©voluer jusquâ€™Ã  ce quâ€™il satisfasse les attentes dÃ©finies.

**ConsÃ©quences**

* Vous gagnez en clartÃ© sur ce que vous attendez vraiment du modÃ¨le.
* Vos prompts deviennent plus robustes, plus rÃ©utilisables et plus faciles Ã  transmettre.
* Vous pouvez constituer une bibliothÃ¨que de prompts testÃ©s, versionnÃ©s et documentÃ©s.
* Le dialogue avec le LLM devient une activitÃ© dâ€™ingÃ©nierie Ã  part entiÃ¨re, structurÃ©e et maÃ®trisÃ©e.

**Exemple**
Une Ã©quipe travaillant sur un assistant de support client voulait gÃ©nÃ©rer automatiquement des rÃ©ponses types Ã  partir de tickets. Le prompt initial donnait des textes trop longs, trop formels et parfois hors sujet. En dÃ©finissant des tests dâ€™attente clairs â€” *"RÃ©ponse â‰¤ 3 phrases, ton empathique mais professionnel, ne jamais inclure dâ€™excuses juridiques"*, etc. â€”, lâ€™Ã©quipe a pu concevoir un prompt beaucoup plus prÃ©cis. Celui-ci a Ã©tÃ© intÃ©grÃ© dans leur base de connaissance, avec des versions pour diffÃ©rents registres de langue selon le client.

**Variations**

* **TDP visuel** : utilisez des "mock outputs" (sorties fictives attendues) comme rÃ©fÃ©rence.
* **TDP collaboratif** : faites dÃ©finir les attentes par plusieurs membres de lâ€™Ã©quipe (produit, technique, UX).
* **TDP dans le prompt** : incluez directement le test dans le prompt lui-mÃªme (ex. : *"Voici un exemple de rÃ©ponse attendue..."*).

<p style="text-align: center;">
    <img src="../images/tdp.png" width="50%" />
</p>

---

