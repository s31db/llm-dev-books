
---
<a id="responsabilite"></a>
## ‚öñÔ∏è Chapitre 7 ‚Äî Responsabilit√©, transparence et limites : une √©thique du d√©veloppement augment√©

> Utiliser un LLM dans le d√©veloppement, ce n‚Äôest pas seulement une opportunit√©. C‚Äôest aussi une responsabilit√©.
> Il ne suffit pas que le r√©sultat fonctionne. Il faut **pouvoir expliquer comment il a √©t√© produit, et √† quelles conditions**.

---

### üß≠ Pourquoi ce chapitre ?

Dans un contexte o√π :

* des outils proposent du code sans auteur clair,
* des √©quipes int√®grent des blocs g√©n√©r√©s sans les comprendre,
* des d√©cisions d‚Äôarchitecture sont prises √† l‚Äôaide de suggestions IA,

la **documentation des interactions avec les LLM** devient un enjeu majeur. Non pas pour tout consigner‚Ä¶ mais pour **rendre visible ce qui a √©t√© g√©n√©r√©, valid√©, interpr√©t√©**.

---

### üìò Documenter l‚Äôusage des LLM

#### Pourquoi documenter ?

* Pour garder une trace des choix faits avec l‚Äôaide de l‚ÄôIA.
* Pour √©viter la **dette g√©n√©rative** : du code produit trop vite, sans explication.
* Pour pouvoir r√©examiner un raisonnement ou un prompt dans six mois.
* Pour outiller les relecteurs et les √©quipes QA.

> La documentation d‚Äôun prompt n‚Äôest pas un luxe. C‚Äôest **une condition de la maintenabilit√©.**

---

#### Que documenter ?

| √âl√©ment                         | Objectif                                                      |
| ------------------------------- | ------------------------------------------------------------- |
| **Prompt source**               | Comprendre l‚Äôintention initiale                               |
| **Version du LLM utilis√©**      | √âvaluer les limites, biais ou hallucinations potentielles     |
| **R√©ponse g√©n√©r√©e**             | Historiser l‚Äôit√©ration utilis√©e                               |
| **Validation humaine apport√©e** | Identifier le r√¥le de l‚Äôhumain dans l‚Äôacceptation du r√©sultat |
| **Hypoth√®ses contextuelles**    | Pr√©server la logique derri√®re la g√©n√©ration                   |

---

#### Formats possibles

* Annotation en commentaire dans le code
* Historique dans l‚Äôoutil LLM (chat, snapshot, fichier `.prompt.md`)
* Documentation √† part (Wiki, PR, fichier `prompts/`)
* Mod√®le structur√© (ex. Fiche Prompt + Tests d‚Äôintention associ√©s)

---

#### Exemple concret

> **Fonction g√©n√©r√©e √† partir d‚Äôun prompt GPT-4 le 12/04/2025**
> 
> Prompt : ‚Äú√âcris une fonction en JavaScript qui valide une adresse mail avec une RegExp simple‚Äù
> 
> R√©ponse modifi√©e pour :
> - Ajouter la gestion des caract√®res sp√©ciaux
> - Remplacer l'alerte par une exception explicite


---

### ‚öñÔ∏è Enjeux √©thiques et responsabilit√©

#### LLM = responsabilit√© partag√©e

> Ce n‚Äôest pas parce qu‚Äôun LLM a propos√© un code que vous en √™tes moins responsable.
> Vous √™tes responsable **de ce que vous comprenez, validez, int√©grez**.

Les mod√®les sont puissants, mais :

* ne donnent aucune garantie de fiabilit√©,
* peuvent reproduire des biais,
* peuvent g√©n√©rer du contenu non conforme ou juridiquement risqu√©,
* ne sont pas capables de refuser une t√¢che inappropri√©e par eux-m√™mes.

> **Un bug venu d‚Äôun exemple convaincant**
>
> Un d√©veloppeur a r√©cemment int√©gr√© un snippet de code g√©n√©r√© par LLM pour l‚Äôauthentification OAuth. Le code √©tait syntaxiquement parfait, comment√©, et semblait s√©curis√©‚Ä¶ sauf qu‚Äôil utilisait une biblioth√®que obsol√®te et vuln√©rable. L‚Äôaudit de s√©curit√© a r√©v√©l√© une faille critique. Le LLM avait simplement ‚Äúrecopi√©‚Äù un exemple dat√©, sans signaler de mise en garde. R√©sultat : plusieurs jours perdus, et une prise de conscience utile.

---

<div class="pb-A4"></div>

#### Risques fr√©quents

| Risque                        | Exemple                                                                  |
|-------------------------------|--------------------------------------------------------------------------|
| **Hallucination de fonction** | Fonction plausible mais non existante dans un langage donn√©              |
| **Copie involontaire**        | Reproduction d‚Äôun bout de code prot√©g√© issu du corpus d‚Äôentra√Ænement     |
| **Biais implicite**           | St√©r√©otypes dans les exemples ou r√©ponses g√©n√©r√©es                       |
| **Surconfiance**              | Prise de d√©cision sans relecture ni test, sur la base d‚Äôun prompt unique |
| **Manque de tra√ßabilit√©**     | Code g√©n√©r√© sans indication de son origine ni de sa validation           |

---

#### Questions √† se poser (checklist √©thique)

1. Ai-je compris ce que le mod√®le a produit ?
2. Puis-je expliquer √† quelqu‚Äôun pourquoi cette solution est valable ?
3. Ai-je test√© ou v√©rifi√© ce code ?
4. Ai-je signal√© qu‚Äôil a √©t√© g√©n√©r√© ?
5. Le mod√®le a-t-il produit une r√©ponse biais√©e ou discutable ?
6. Cette interaction pourrait-elle √™tre mal interpr√©t√©e ou mal r√©utilis√©e par quelqu‚Äôun d‚Äôautre ?
7. Est-ce que j‚Äôassumerais cette d√©cision en production ?

> Si la r√©ponse est ‚Äúnon‚Äù √† deux questions ou plus, il est **trop t√¥t pour valider cette contribution IA.**

---

### üîç Vers une culture de la transparence

* Rendre visible l‚Äôusage des LLM n‚Äôest pas une contrainte. C‚Äôest **un levier de confiance collective.**
* Cela permet de relire, de corriger, de transmettre.
* Cela constitue une **preuve de diligence technique** en cas de litige ou d‚Äôincident.
* Cela alimente une culture d‚Äô√©quipe o√π l‚ÄôIA **stimule le raisonnement plut√¥t qu‚Äôelle ne le remplace**.

---

> **Le ‚ÄúJournal du dialogue‚Äù**
>
> Dans une startup du secteur sant√©, chaque interaction avec un LLM pour des sujets critiques (protocoles, anonymisation, s√©curit√©) est archiv√©e sous forme de journal. Ce journal inclut : prompt initial, it√©rations, choix retenus, √©valuation humaine, et justification des d√©cisions. Ce dispositif am√©liore la transparence interne, facilite les audits, et cultive une posture r√©flexive.

---

<div class="pb-A4"></div>

### üîê Prot√©ger les donn√©es, m√™me dans le dialogue

> *Tout ce que vous envoyez √† un LLM n‚Äôest pas neutre ‚Äî ni invisible.*

Les interactions avec un LLM peuvent exposer involontairement des donn√©es sensibles, confidentielles ou personnelles : noms de clients, extraits de code propri√©taire, exemples de production, ou encore d√©cisions strat√©giques.

M√™me lorsque l‚Äôoutil semble local ou ¬´ s√©curis√© ¬ª, il est essentiel d‚Äôadopter une posture de prudence active :

* **Filtrer en amont** les donn√©es transmises, comme on le ferait pour une publication publique.
* **√âviter les copier-coller aveugles** issus de documents confidentiels ou de bases internes.
* **Utiliser des environnements contr√¥l√©s**, capables de garantir la non-exploitation des donn√©es (LLM auto-h√©berg√©s, mode entreprise, clauses contractuelles explicites).
* **Anonymiser les donn√©es** utilis√©es dans les prompts, d√®s que possible.
* **Former les √©quipes** aux risques li√©s √† la fuite involontaire d‚Äôinformation via un prompt mal formul√©.

Enfin, se poser une question simple avant chaque envoi‚ÄØ:

> *‚ÄúAurais-je le droit d‚Äôenvoyer ceci par email √† une tierce personne ext√©rieure √† mon organisation ?‚Äù*
> Si la r√©ponse est non, alors le prompt doit √™tre retravaill√©.

Ce souci de **protection des donn√©es** s‚Äôinscrit dans une √©thique plus large‚ÄØ: celle d‚Äôun d√©veloppement **responsable, tra√ßable et conscient de ses impacts** ‚Äî techniques, sociaux et l√©gaux.

### ‚úèÔ∏è En r√©sum√©

* La documentation des prompts et des interactions est une **bonne pratique technique** et un **geste √©thique.**
* Les LLM d√©placent la responsabilit√©, mais ne la dissolvent pas.
* Seule une **pratique transparente et partag√©e** peut garantir la qualit√©, la robustesse et l‚Äô√©thique des conceptions assist√©es par IA.

> Les LLM ne pensent pas. Ils compl√®tent.
> Mais vous, vous **pensez avec eux** ‚Äî et cela vous engage.
