
---

## âš–ï¸ Chapitre 7 â€” ResponsabilitÃ©, transparence et limites : une Ã©thique du dÃ©veloppement augmentÃ©

> Utiliser un LLM dans le dÃ©veloppement, ce nâ€™est pas seulement une opportunitÃ©. Câ€™est aussi une responsabilitÃ©.
> Il ne suffit pas que le rÃ©sultat fonctionne. Il faut **pouvoir expliquer comment il a Ã©tÃ© produit, et Ã  quelles conditions**.

---

## ðŸ§­ Pourquoi ce chapitre ?

Dans un contexte oÃ¹ :

* des outils proposent du code sans auteur clair,
* des Ã©quipes intÃ¨grent des blocs gÃ©nÃ©rÃ©s sans les comprendre,
* des dÃ©cisions dâ€™architecture sont prises Ã  lâ€™aide de suggestions IA,

la **documentation des interactions avec les LLM** devient un enjeu majeur. Non pas pour tout consignerâ€¦ mais pour **rendre visible ce qui a Ã©tÃ© gÃ©nÃ©rÃ©, validÃ©, interprÃ©tÃ©**.

---

## ðŸ“˜ Partie 1 â€” Documenter lâ€™usage des LLM

### 1.1 Pourquoi documenter ?

* Pour garder une trace des choix faits avec lâ€™aide de lâ€™IA.
* Pour Ã©viter la **dette gÃ©nÃ©rative** : du code produit trop vite, sans explication.
* Pour pouvoir rÃ©examiner un raisonnement ou un prompt dans six mois.
* Pour outiller les relecteurs et les Ã©quipes QA.

> La documentation dâ€™un prompt nâ€™est pas un luxe. Câ€™est **une condition de la maintenabilitÃ©.**

---

### 1.2 Que documenter ?

| Ã‰lÃ©ment                         | Objectif                                                      |
| ------------------------------- | ------------------------------------------------------------- |
| **Prompt source**               | Comprendre lâ€™intention initiale                               |
| **Version du LLM utilisÃ©**      | Ã‰valuer les limites, biais ou hallucinations potentielles     |
| **RÃ©ponse gÃ©nÃ©rÃ©e**             | Historiser lâ€™itÃ©ration utilisÃ©e                               |
| **Validation humaine apportÃ©e** | Identifier le rÃ´le de lâ€™humain dans lâ€™acceptation du rÃ©sultat |
| **HypothÃ¨ses contextuelles**    | PrÃ©server la logique derriÃ¨re la gÃ©nÃ©ration                   |

---

### 1.3 Formats possibles

* Annotation en commentaire dans le code
* Historique dans lâ€™outil LLM (chat, snapshot, fichier `.prompt.md`)
* Documentation Ã  part (Wiki, PR, fichier `prompts/`)
* ModÃ¨le structurÃ© (ex. Fiche Prompt + Tests dâ€™intention associÃ©s)

---

### 1.4 Exemple concret

```js
// Fonction gÃ©nÃ©rÃ©e Ã  partir dâ€™un prompt GPT-4 le 12/04/2025
// Prompt : â€œÃ‰cris une fonction en JavaScript qui valide une adresse mail avec une RegExp simpleâ€
// RÃ©ponse modifiÃ©e pour :
  // - Ajouter la gestion des caractÃ¨res spÃ©ciaux
  // - Remplacer l'alerte par une exception explicite
```

---

## âš–ï¸ Partie 2 â€” Enjeux Ã©thiques et responsabilitÃ©

### 2.1 LLM = responsabilitÃ© partagÃ©e

> Ce nâ€™est pas parce quâ€™un LLM a proposÃ© un code que vous en Ãªtes moins responsable.
> Vous Ãªtes responsable **de ce que vous comprenez, validez, intÃ©grez**.

Les modÃ¨les sont puissants, mais :

* ne donnent aucune garantie de fiabilitÃ©,
* peuvent reproduire des biais,
* peuvent gÃ©nÃ©rer du contenu non conforme ou juridiquement risquÃ©,
* ne sont pas capables de refuser une tÃ¢che inappropriÃ©e par eux-mÃªmes.

> **Un bug venu dâ€™un exemple convaincant**
>
> Un dÃ©veloppeur a rÃ©cemment intÃ©grÃ© un snippet de code gÃ©nÃ©rÃ© par LLM pour lâ€™authentification OAuth. Le code Ã©tait syntaxiquement parfait, commentÃ©, et semblait sÃ©curisÃ©â€¦ sauf quâ€™il utilisait une bibliothÃ¨que obsolÃ¨te et vulnÃ©rable. Lâ€™audit de sÃ©curitÃ© a rÃ©vÃ©lÃ© une faille critique. Le LLM avait simplement â€œrecopiÃ©â€ un exemple datÃ©, sans signaler de mise en garde. RÃ©sultat : plusieurs jours perdus, et une prise de conscience utile.

---

### 2.2 Risques frÃ©quents

| Risque                        | Exemple                                                                  |
|-------------------------------|--------------------------------------------------------------------------|
| **Hallucination de fonction** | Fonction plausible mais non existante dans un langage donnÃ©              |
| **Copie involontaire**        | Reproduction dâ€™un bout de code protÃ©gÃ© issu du corpus dâ€™entraÃ®nement     |
| **Biais implicite**           | StÃ©rÃ©otypes dans les exemples ou rÃ©ponses gÃ©nÃ©rÃ©es                       |
| **Surconfiance**              | Prise de dÃ©cision sans relecture ni test, sur la base dâ€™un prompt unique |
| **Manque de traÃ§abilitÃ©**     | Code gÃ©nÃ©rÃ© sans indication de son origine ni de sa validation           |

---

### 2.3 Questions Ã  se poser (checklist Ã©thique)

1. Ai-je compris ce que le modÃ¨le a produit ?
2. Puis-je expliquer Ã  quelquâ€™un pourquoi cette solution est valable ?
3. Ai-je testÃ© ou vÃ©rifiÃ© ce code ?
4. Ai-je signalÃ© quâ€™il a Ã©tÃ© gÃ©nÃ©rÃ© ?
5. Le modÃ¨le a-t-il produit une rÃ©ponse biaisÃ©e ou discutable ?
6. Cette interaction pourrait-elle Ãªtre mal interprÃ©tÃ©e ou mal rÃ©utilisÃ©e par quelquâ€™un dâ€™autre ?
7. Est-ce que jâ€™assumerais cette dÃ©cision en production ?

> Si la rÃ©ponse est â€œnonâ€ Ã  deux questions ou plus, il est **trop tÃ´t pour valider cette contribution IA.**

---

## ðŸ” Vers une culture de la transparence

* Rendre visible lâ€™usage des LLM nâ€™est pas une contrainte. Câ€™est **un levier de confiance collective.**
* Cela permet de relire, de corriger, de transmettre.
* Cela constitue une **preuve de diligence technique** en cas de litige ou dâ€™incident.
* Cela alimente une culture dâ€™Ã©quipe oÃ¹ lâ€™IA **stimule le raisonnement plutÃ´t quâ€™elle ne le remplace**.

---

> **Le â€œJournal du dialogueâ€**
>
> Dans une startup du secteur santÃ©, chaque interaction avec un LLM pour des sujets critiques (protocoles, anonymisation, sÃ©curitÃ©) est archivÃ©e sous forme de journal. Ce journal inclut : prompt initial, itÃ©rations, choix retenus, Ã©valuation humaine, et justification des dÃ©cisions. Ce dispositif amÃ©liore la transparence interne, facilite les audits, et cultive une posture rÃ©flexive.

---


## ðŸ” ProtÃ©ger les donnÃ©es, mÃªme dans le dialogue

> *Tout ce que vous envoyez Ã  un LLM nâ€™est pas neutre â€” ni invisible.*

Les interactions avec un LLM peuvent exposer involontairement des donnÃ©es sensibles, confidentielles ou personnelles : noms de clients, extraits de code propriÃ©taire, exemples de production, ou encore dÃ©cisions stratÃ©giques.

MÃªme lorsque lâ€™outil semble local ou Â« sÃ©curisÃ© Â», il est essentiel dâ€™adopter une posture de prudence active :

* **Filtrer en amont** les donnÃ©es transmises, comme on le ferait pour une publication publique.
* **Ã‰viter les copier-coller aveugles** issus de documents confidentiels ou de bases internes.
* **Utiliser des environnements contrÃ´lÃ©s**, capables de garantir la non-exploitation des donnÃ©es (LLM auto-hÃ©bergÃ©s, mode entreprise, clauses contractuelles explicites).
* **Anonymiser les donnÃ©es** utilisÃ©es dans les prompts, dÃ¨s que possible.
* **Former les Ã©quipes** aux risques liÃ©s Ã  la fuite involontaire dâ€™information via un prompt mal formulÃ©.

Enfin, se poser une question simple avant chaque envoiâ€¯:

> *â€œAurais-je le droit dâ€™envoyer ceci par email Ã  une tierce personne extÃ©rieure Ã  mon organisation ?â€*
> Si la rÃ©ponse est non, alors le prompt doit Ãªtre retravaillÃ©.

Ce souci de **protection des donnÃ©es** sâ€™inscrit dans une Ã©thique plus largeâ€¯: celle dâ€™un dÃ©veloppement **responsable, traÃ§able et conscient de ses impacts** â€” techniques, sociaux et lÃ©gaux.

## âœï¸ En rÃ©sumÃ©

* La documentation des prompts et des interactions est une **bonne pratique technique** et un **geste Ã©thique.**
* Les LLM dÃ©placent la responsabilitÃ©, mais ne la dissolvent pas.
* Seule une **pratique transparente et partagÃ©e** peut garantir la qualitÃ©, la robustesse et lâ€™Ã©thique des conceptions assistÃ©es par IA.

> Les LLM ne pensent pas. Ils complÃ¨tent.
> Mais vous, vous **pensez avec eux** â€” et cela vous engage.
