
---

### ğŸŸ£ Pattern 4 â€” **Mirror Model**: *Compare to Clarify a Choice*

<p style="text-align: center;">
    <img src="../images/motif_miroir_en.png" width="50%" />
</p>

**ğŸ¯ Context**
Youâ€™re hesitating between several possible solutions: two architectures, two algorithmic approaches, two coding styles, two tools. The team is discussing, but the debate stays vague or biased. You need to step back and decide *consciously* â€” not by reflex or personal preference.

**ğŸš§ Problem**
By default, an LLM often replies with *a single solution*. Yet in complex situations itâ€™s more useful to **compare several options** than to get one â€œtypical answer.â€ Without confronting alternatives, you risk sticking with the first good-sounding proposalâ€¦ and missing the consequences.

**âœ… Solution**
Use the LLM as a **comparative mirror**: explicitly ask it to produce several variants of a solution, then to compare them according to defined criteria (readability, performance, maintainability, UXâ€¦). This turns the answer into a **dialectical analysis** that illuminates the decision.

> Example prompts:
>
> * â€œProvide two implementations of this function â€” one imperative, one functional â€” and compare them.â€
> * â€œGive three architecture options and their pros/cons given our constraints.â€
> * â€œCompare React and Svelte for this type of project.â€

<div class="pb-A4"></div>

**ğŸ“Œ Consequences**

* Promotes critical analysis instead of mimicry.
* Makes the choice criteria explicit.
* Supports collective decision-making, especially in a team context.
* Reduces confirmation or authority bias.
* Serves as a basis for documenting decisions.

**ğŸ’¡ Example Use**
In a payment-system redesign project, the team is hesitating between:

1. An event-driven architecture with Kafka
2. A more classic synchronous REST architecture

The prompt becomes:

> *â€œCompare these two options for a high-availability system handling 100 transactions per second. What are the trade-offs?â€*

The LLM highlights that:

* Kafka is more resilient but harder to monitor,
* REST is simpler to test but less robust to load spikes.

The discussion then draws on these elements to make an **argued decision**, not simply â€œbecause weâ€™ve always done it that way.â€

#### **ğŸŒ€ Useful Variants**

* **Style Mirror**: compare imperative vs functional style, object-oriented vs declarative.
* **Paradigm Mirror**: polling vs event-driven, synchronous vs asynchronous.
* **Tool Mirror**: front-end frameworks, database engines, testing libraries, etc.
* **UX Mirror**: compare two error messages, two user journeys.

**ğŸ› ï¸ Associated Tools**

* A comparison grid co-built with the LLM.
* A two-dimensional table: options Ã— criteria.
* Possible integration into an Architecture Decision Record (ADR) doc.

**ğŸ§  Recommended Posture**
Ask for *several options* before diving into just one. Make the LLM a **stimulator of reasoned divergence**. It wonâ€™t decide for you â€” it will light the way.

**ğŸ’¬ Prompt to Remember**

> *â€œPropose several alternatives for this need, then compare them according to these criteria: \[X, Y, Z].â€*
