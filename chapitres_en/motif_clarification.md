
---

### ğŸŸ£ Pattern 5 â€” **Clarification by Counterexample**: *Probe the Limits of a Proposal*

<p style="text-align: center;">
    <img src="../images/motif_clarification_en.png" width="50%" />
</p>

**ğŸ¯ Context**
The LLM has produced a satisfactory answer â€” code, a technical solution, a recommendation. Everything looks fineâ€¦ but a nagging doubt remains. Is it really robust? Does the answer cover all cases? Does the reasoning hold up at the edges?

**ğŸš§ Problem**
The model often produces an â€œidealâ€ or typical solution that **hides edge cases or failure modes**. The developer may be tempted to trust it by default. Yet without putting it to the test, you risk deploying a fragile, biased, or naÃ¯ve solution.

**âœ… Solution**
Question the answer **by negation**: ask for a **counterexample**, a situation where the solution fails, becomes inefficient, or produces an unexpected effect. This reveals the **implicit limits** of the reasoning and sharpens your understanding of what the solution does â€” and doesnâ€™t â€” cover.

> Example prompts:
>
> * â€œIn what case could this solution fail?â€
> * â€œCan you propose an example of data that would cause a problem?â€
> * â€œWhat if the file is empty? If the connection fails? If the user isnâ€™t authenticated?â€
> * â€œWhich implicit assumption, if false, would make this solution invalid?â€

**ğŸ“Œ Consequences**

* Early detection of edge cases.
* Greater robustness of the proposed solution.
* Fosters a critical mindset in the developer.
* Reduces side effects or surprises in production.
* Enriches the initial prompt if needed (see Pattern 6).

<div class="pb-A4"></div>

**ğŸ’¡ Example Use**
A student asks the LLM to implement Dijkstraâ€™s algorithm in JavaScript.
The solution looks correct. He follows up with:

> *â€œWhat if the graph contains negative cycles?â€*

The LLM replies:

> *â€œDijkstra isnâ€™t suited for that case. Youâ€™d need Bellmanâ€“Ford, which handles negative weights.â€*

This simple follow-up turns a generation session into a **moment of algorithmic learning**, by making an invisible assumption visible.

#### **ğŸŒ€ Useful Variants**

* **Boundary Test**: â€œWhat if the array is empty? If a value is null?â€
* **Stress Test**: â€œWhat if 10,000 users access this module at the same time?â€
* **Business Counter-Rule**: â€œWhat business situation would invalidate this rule?â€
* **Simulated Debate**: â€œCan you simulate the opinion of a developer who criticizes this solution?â€

**ğŸ› ï¸ Associated Tools**

* Acceptance-test tables enriched by the model.
* Combined use with test-data generation (see Pattern 3).
* Augmented pairing: a developer plays devilâ€™s advocate with the LLM.

**ğŸ§  Recommended Posture**
Donâ€™t be satisfied with the â€œapparently goodâ€ answer. Adopt a **scientific posture**: falsify, test, push the logic to its edges. This is how the LLM becomes a **critical partner**, not a flattering automaton.

**ğŸ’¬ Prompt to Remember**

> *â€œGive a case that would make this solution fail. What does that reveal about its limits?â€*
