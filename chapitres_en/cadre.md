
---
<a id="cadre"></a>
## ğŸ”¬ Chapter 9 â€” Implementation Frameworks: Workshops, Methods, and Rituals for an Augmented Practice

> Hereâ€™s the testing ground: formats for learning together, exploring, testing, documenting, and transmitting AI usage within your teams.

After exploring motifs, principles, and scenarios of augmented development, this chapter offers **concrete formats** to integrate these practices into the daily reality of teams. Workshops, rituals, canvases, serious games: the aim is to make the contributions of LLMs tangible within safe, learning-oriented collective dynamics.

---

### âœï¸ 1. â€œTeam Prompt Designâ€ Workshop

> **Objective:** Learn to formulate, reformulate, and test prompts collectively in order to explore a real topic and improve the quality of interactions with LLMs.

---

**ğŸ•’ Duration:** 1h30 to 2h

**ğŸ‘¥ Participants:** 3 to 6 people (developers, PO, UX, QA, facilitatorâ€¦)

**ğŸ§° Materials:** access to an LLM, prompt canvas (paper or Miro), space to visualize responses

---

**ğŸ” Typical Flow**

#### Introduction & Framing (10 min)

Present the workshop objective:
*â€œExplore collectively how to better formulate our prompts for a real case.â€*

Briefly explain expected postures: openness, iteration, non-judgment

Choose a real topic or case together:

* module breakdown
* test formulation
* technical choice
* reframing a user need

---

#### Initial Prompt (15 min)

Write a **first naive prompt** together: â€œWhat would we ask an LLM in this context?â€

<div class="pb-A4"></div>  

Read the generated response.

Identify potential problems:

* vagueness, ambiguities, imprecision
* terms too technical or undefined
* unspoken or implicit intention

---

#### Iterations & Reformulations (30â€“40 min)

Reformulate the prompt from different angles or strategies:

* explicit role (e.g., â€œYou are a software architectâ€¦â€)
* step by step
* structured version / bullet points
* critical / exploratory / generative version

For each version:

* LLM generates a response
* Quick discussion: how is it different? more useful? biased?

If useful: directly compare several formulations with the same model.

---

#### Extracting a Prompt Pattern (15â€“20 min)

From the tested versions, collectively formalize a **reusable prompt pattern**:

* base structure
* optional variants or modules
* conditions of use
* pitfalls to avoid

Document everything in a team canvas or library.

---

#### Retrospective & Learnings (10â€“15 min)

Quick roundtable:

* What I learned
* What Iâ€™ll reuse tomorrow
* What Iâ€™d still like to test

Possible decision:

* publish a cleaned-up version of the prompt
* test this prompt on other similar cases
* surface an **interaction motif** to add to the pattern language

---

<div class="pb-A4"></div>

> **ğŸ§  Summary:**
>
> * Structuring workshop to build collective capacity for good formulation
> * Allows comparing, critiquing, and improving LLM interactions
> * Generates useful, reusable prompts adapted to the team

> âš ï¸ **Pitfalls to Avoid:**
>
> * Focusing on â€œtheâ€ right answer instead of testing variations
> * Failing to name hidden intentions behind a prompt
> * Letting one person write while everyone else just watches

---

### ğŸ—£ï¸ 2. â€œDialogue Dailyâ€ Ritual

> **Objective:** Establish a short, informal, regular ritual where each team member shares their most notable interactions with an LLM. Encourages collective learning, vigilance, and inspiration.

---

**ğŸ•’ Duration:** 5 to 10 minutes

**ğŸ‘¥ Participants:** the whole team (dev, PO, UX, QA, facilitatorâ€¦)

**ğŸ“† Frequency:** daily or twice-weekly (adapt to pace and usage)

**ğŸ§° Optional Support:** prompt wall, dedicated Slack channel, shared board

---

**ğŸ” Typical Flow (per session)**

#### Introduction (1 min)

Short opening remark (facilitator or volunteer):
â€œWhat did AI teach us today?â€

Remind the **3 guiding questions** (posted or verbal):

* **ğŸ§ª What did I try with an LLM?**
* **ğŸ˜² What surprised, helped, or disappointed me?**
* **ğŸ“Œ What do I take away or want to try next?**

---

#### Spontaneous or Rotating Shares (5â€“8 min)

One to three people briefly share a notable interaction:

* success or failure
* interesting prompt
* observed bias
* strange or brilliant answer
* off-label LLM usage

Others can react, ask questions, or add anecdotes.

> ğŸ“ *If no one shares spontaneously, draw a motif or â€œprompt of the dayâ€ card for inspiration.*

---

#### Closing & Capture (1â€“2 min)

Team chooses one or two points to **retain or capture**:

* Add a card to the â€œprompt grimoireâ€
* Note a frequent mistake or good practice
* Propose a test for the next sprint

Update the shared support:

* Experimentation dashboard
* Slack thread â€œ#daily-llmâ€
* Miro/Notion logbook

---

> **ğŸ§  Summary:**
>
> * Simple, lightweight ritual with no prep
> * Surfaces real usage, pitfalls, new ideas
> * Feeds ongoing team knowledge capture
> * Fosters a culture of experimentation and horizontal learning

> âš ï¸ **Pitfalls to Avoid:**
>
> * Turning it into a static stand-up or forced roundtable
> * Slipping into judgment or competition over â€œbest promptsâ€
> * Not connecting learnings to action (tests, documentationâ€¦)
> * Not planning a place to capture useful stories

---

### ğŸ—ºï¸ 3. â€œDialogue Motif Mappingâ€ Workshop

> **Objective:** Identify the most useful, frequent, or desirable LLM interaction motifs for the team and turn them into a shared base to guide future usage.

---

**ğŸ•’ Duration:** 1h30 to 2h

**ğŸ‘¥ Participants:** 4 to 8 people (developers, PO, UX, testing, facilitatorsâ€¦)

**ğŸ§° Materials:**

* Motif cards or sheets (from the book or local practices)
* Double-entry board (frequency / usefulness)
* Collection space (Miro, whiteboard, wikiâ€¦)

---

<div class="pb-A4"></div>

**ğŸ” Workshop Flow**

#### Introduction & Framing (10 min)

Remind what an **LLM interaction motif** is: a recurring form of use with intent, structure, and effect.
Why map them? *To orient, inspire, transmit, progress.*
Present the mapping support: a double-entry matrix *(X-axis: frequency of use; Y-axis: perceived usefulness)*

---

#### Re-activating Known Motifs (15 min)

Quick reading or visual presentation of 6â€“10 existing motifs.
For each motif:

* Team says if they know it
* If theyâ€™ve used it, and in what context

**Examples of motifs:**

Rephrasing a fuzzy idea
Generating test cases
Exploring architecture alternatives
Translating a business need into a user story
Step-by-step explanation of a behavior

---

#### Collective Mapping (30 min)

> Place motifs on the matrix in two steps.

**Individual or Pair Work (10 min)**
Each participant places motifs on the matrix according to:

* Frequency in their daily work
* Perceived usefulness

**Group Discussion (20 min)**

* Compare positions
* Consensus or dispersion: where is there agreement or divergence?
* Note open questions or underused motifs

---

<div class="pb-A4"></div>

#### Generating New Motifs (20 min)

From recent usage or â€œgapsâ€ in the matrix:

* Which interaction types are missing from the map?
* What have we seen work but not yet formalized?

Each participant or sub-group sketches a **new motif** on a blank card:

* Intent
* Prompt structure
* Examples
* Limits or pitfalls

---

#### Consolidation & Capture (15 min)

* Gather all cards/motifs on a common support (wall, digital board)
* Suggest sorting or grouping by family: *exploration*, *reduction*, *control*, *creation*, etc.
* Agree on what to publish / share / test further

---

#### Bonus (optional)

* Give each motif an original name (â€œThe Socratic Coach,â€ â€œThe Clever Counter-Example,â€ etc.)
* Vote on motifs to formalize in the team library or repository

---

> **ğŸ§  Summary:**
>
> * Creates a shared view of useful forms of dialogue with an LLM
> * Surfaces dominant usagesâ€¦ and blind spots
> * Gives a starting point for motifs to formalize or spread

> âš ï¸ **Pitfalls to Avoid:**
>
> * Talking only about technical (or only functional) motifs
> * Underestimating postures (curiosity, prudence, critiqueâ€¦)
> * Reducing the map to a â€œbest promptsâ€ ranking
> * Underestimating the need for collective reformulation

---

<div class="pb-A4"></div>

### ğŸ² 4. The â€œAbsurd Promptsâ€ Game

> **Objective:** Experiment with the limits, paradoxes, hallucinations, and biases of language models â€” with humor and critical thinking.

---

**ğŸ•’ Duration:** 1h to 1h30

**ğŸ‘¥ Participants:** 4 to 10 people

**ğŸ§° Materials:** access to an LLM, sticky notes or shared board, capture tool (Miro, Notion, whiteboardâ€¦)

---

**ğŸ” Typical Flow**

#### Introduction (10 min)

* Present the goal of the workshop: *â€œPlay with the limits to better understand them.â€*
* Explain the rules: create absurd prompts, LLM answers seriously, then analyze.
* Remind expected postures: kindness, curiosity, constructive critique, no mocking people.

---

#### Group Warm-Up (10 min)

* Each participant invents an **absurd, contradictory, or vague prompt** (e.g., â€œWrite a poem about a programming language that doesnâ€™t exist but has bugs.â€)
* Read a few examples aloud.
* Group picks 2â€“3 to submit to the LLM to start.

---

#### Prompt Creation & Selection (15â€“20 min)

Each person writes 2 prompts:

* one deliberately paradoxical or fallacious
* one inspired by an error or bad formulation already encountered

Pool them: participants read their proposals aloud.

Group selects 3â€“5 prompts to test based on:

* potential for derailment or surprise
* link to realistic professional situations

---

<div class="pb-A4"></div>

#### Dialogue With the LLM (20â€“30 min)

Submit prompts one by one to the LLM.
For each response:

* Collective reading
* Guided debrief:

  * What did the model attempt?
  * What does this reveal about its functioning?
  * Bug or over-obedience?
  * What risks if this response were taken seriously?

---

#### Collective Synthesis (15 min)

In group or pairs: what types of errors did we observe?

* Hallucinations?
* Absurd but credible answers?
* Blind obedience to incoherent orders?
* Lack of ethical or logical filter?

Capture on a shared board:

* â€œWhat this teaches me about LLMsâ€
* â€œWhat this teaches me about my formulationâ€

---

#### (Optional) Educational Variant

Create a â€œFictional but Plausible Errorâ€ sheet:

* Initial prompt
* Absurd response
* Risk if taken seriously
* Good reflex for review or reformulation

---

> **ğŸ§  Summary:**
>
> * Playful workshop to sharpen critical thinking
> * Lets you discuss LLM flaws without pressure
> * Builds a culture of doubt and reformulation in the team

> âš ï¸ **Pitfalls to Avoid:**
>
> * Laughing at colleaguesâ€™ errors instead of analyzing formulations
> * Believing this game replaces serious testing practice
> * Forgetting to draw applicable lessons for real contexts

---

<div class="pb-A4"></div>

### ğŸ“˜ 5. â€œLLM Readyâ€ Team Reference

> **Objective:** Co-create an LLM usage guide adapted to the team, grounded in real experience, needs, and collective learning.

---

**ğŸ•’ Duration:** 2h (can be split into 2 Ã— 1h sessions)

**ğŸ‘¥ Participants:** whole team or volunteer sub-group (4 to 8 people)

**ğŸ§° Materials:**

* Miro / whiteboard or physical wall
* Access to an LLM interaction history (if available)
* Reference template (Notion, markdown, wikiâ€¦)

---

**ğŸ” Workshop Flow**

#### Introduction & Goals (10 min)

Why create a reference? *To capitalize, transmit, secure, save time*
Reminder: this is **not a fixed standard**, but an **evolving support**
Quick presentation of possible sections: prompt types, rules, pitfalls, validation levelsâ€¦

---

#### Sharing Concrete Usages (20 min)

> Which LLM interactions were really helpful â€” or problematic?

Each person shares **1â€“2 notable examples** (successes or failures)
Quick writing in pairs or on sticky notes:

* Context
* Prompt
* Result
* Learning

Collectively classify into three columns:

* ğŸ” To reproduce
* âš ï¸ To adapt
* ğŸ›‘ To avoid

---

<div class="pb-A4"></div>

#### Building the Reference (45 min)

> Build sections from real stories.

##### **Prompt Types**

Extract reusable effective formulations
Organize by use: writing, code analysis, transformation, explorationâ€¦

##### **Response Quality Criteria**

Propose a **common grid**:

* Relevance
* Robustness
* Transparency
* Security
* Alignment with team standards

##### **Usage Rules**

Define clear, simple rules together:

* When to use an LLM
* When to validate with a human
* When to document the response

##### **Blacklist / Frequent Pitfalls**

Capture encountered errors: vague prompts, credible hallucinations, overconfidence, etc.

---

#### Formatting & Publishing (15 min)

Choose the publication format: Notion, README, Miro, Confluence pageâ€¦
Assign roles:

* 1 reference steward
* 1â€“2 evolution keepers (e.g., sprint review, retro)

---

#### Retrospective & Commitment (10 min)

Roundtable:

* â€œWhat I learnedâ€
* â€œWhat I want to test nowâ€
* â€œWhat Iâ€™d like in the next versionâ€

Reminder: a reference is **never finished**, it **co-evolves** with the team.

---

> **ğŸ§  Summary:**
>
> * Structuring workshop to stabilize good AI practices in the team
> * Creates a useful, evolving, appropriated reference
> * Strengthens collective reflexivity and quality of usage

> âš ï¸ **Pitfalls to Avoid:**
>
> * Writing a â€œtheoreticalâ€ reference disconnected from real usage
> * Freezing it as a rigid standard
> * Letting it age without regular review (plan an update rhythm)

---

### Augmented Engineering Is Also Social Engineering

These formats show that augmented development isnâ€™t just about tooling. It relies on:

* a culture of dialogue (with AI and between humans),
* the ability to make our reasoning explicit,
* a reflective practice that transforms the team as much as the deliverables.

> ğŸ§µ **Key Takeaway:**
> This chapter is an open toolbox. Each proposed format can be adapted, combined, repurposed. What matters is not applying them â€œto the letter,â€ but making them your own to create your own paths toward an augmented, collective, and responsible coding practice.
